#### 使用Tkinter库制作相应的GUI界面

### Tkinter库的使用

感觉AIGC只能帮忙写这个

```python
from tkinter import *
#创建窗口
window = Tk()
window.title("First Window")
window.geometry("350x200")
lbl = Label(window, text="Hello",font("Arial Bold",50))
#必须grid否则不会显示
lbl.grid(column=0, row=0)
#按钮
btn = Button(window, text="Click Me", bg="orange", fg="red")
btn.grid(column=1, row=0)
#打开窗口的操作
window.mainloop()
```



#### 爬虫任务可以被分解成哪几个小任务

##### 1.先检查是否有API

> 有API则直接调用API,无API则爬虫

所以去查了一下资料,发现洛谷虽然没有API可以直接调用,但是几乎完全没有反爬虫,所以应该会轻松一些

==后面才意识到其实是有API的,之前对请求调用的认知太过于浅薄==

![image-20230914164521305](D:\Database\MarkdownImage\image-20230914164521305.png)

后期开始制作按需爬取的时候才意识到问题

洛谷的题库URL逻辑:https://www.luogu.com.cn/problem/P

解答的URL是https://www.luogu.com.cn/problem/solution/P

爬取解答需要伪装登录状态

![image-20230909191742866](D:\Database\MarkdownImage\image-20230909191742866.png)

![image-20230909194135067](D:\Database\MarkdownImage\image-20230909194135067.png)

这大概就是解答所需要解包的东西

P后面的数字对应题号,只要向该URL发送GET请求就可以获得html内容

##### 2.数据结构分析与数据存储

> 确定所需字段
>
> 确定构建的表及连接关系
>
> > **根据作业要求应改为对应的md文件和文件夹存储**
>
> 选择数据库

##### 3.数据流分析

> 确定采集范围以及切入源头
>
> 多层网页结构间跳转流程
>
> 范围细分
>
> 访问方式分析
>
> URL及参数分析

##### 4.数据采集

>常用的模块
>
>> requests
>>
>> urllib

```python
#使用urllib库进行请求遇到HTTP代码302重定向的问题,发现Requests库似乎更加好用
#urllib.error.HTTPError: HTTP Error 302: The HTTP server returned a redirect error that would lead to an infinite loop.
The last 30x error message was:
Moved Temporarily
#服务器响应30x会自动跟随重定向,返回结果是重定向之后的结果而不是30x的响应效果
#使用Requests库触发403,直接拒绝访问,IP被ban,过一段时间才行
```

在试图爬取解答的时候发现解答是加密的

![image-20230910135751849](D:\Database\MarkdownImage\image-20230910135751849.png)

JSON.parse(decodeURIComponent())

查了下是JavaScript的一个函数,可把字符串作为URI组件进行编码

URI(Uniform Resource Locator)唯一资源定位符

使用unquote解码(默认是UTF-8)

![image-20230910145524250](D:\Database\MarkdownImage\image-20230910145524250.png)

似乎是Unicode编码

正确解码后爬得

![image-20230910155105052](D:\Database\MarkdownImage\image-20230910155105052.png)

发现需要获取的实际内容是一次赋值的操作

如果要通过json的方式去提取需要一些处理

我在想是否有更好的提取方式

>> scrapy
>
>解析工具
>
>> BeautifulSoup
>
>> 正则表达式
>
>数据整理
>
>**在这里有一步提取标题并放在文件目录中**
>
>标题存放在article字段下的h1
>
>>字符串方法
>
>>正则表达式
>
>>Pandas
>
>写入数据库
>
>爬虫效率提升
>
>> 减少访问次数
>
>> > 去重,机器学习
>
>> 多线程/多进程/协程
>
>> 分布式爬虫
>
>数据质量管理
>
>> 断点续传,错漏校验

##### 5.反反爬虫(因为洛谷的原因所以基本不需要考虑这个问题)

> 加headers
>
> 随机延时
>
> IP代理池
>
> 帐号池
>
> 模拟浏览器
>
> 验证码

##### 6.爬虫的道德节操和法律问题

> 控制采集速度
>
> 注意商业用途

```python
#已经可以爬出题目和解答,解答的部分有缺陷
#我把整个爬虫封装成了一个需要传两个参数的函数
#一个是爬的起始点,一个是爬的终点
#不过如果是要根据题库难度进行爬取的话可能没那么简单
#最理想化的办法是将不同难度对应的题号封装在一个list里面,按照list去爬
#不过实现起来估计需要另一个爬虫来爬难度对应的题号
#
#GUI没做,因为g
import re
import requests
import bs4
import urllib.parse
import json


baseUrl = "https://www.luogu.com.cn/problem"
savePath = "D:\\Database\\1261640141\\FileRecv\\Term3\\软件工程\\洛谷题库\\"
minnum = 1020
maxnum = 1080

def main():
    crawler(minnum,maxnum)

def crawler(minnum,maxnum):
    print("准备爬取P{}".format(maxnum))
    for i in range(minnum,maxnum+1):
        print("正在爬取P{}...".format(i),end="")
        r = getProblemHTML(baseUrl +"/P" +str(i))
        if str(r).find("200") != 11:
            print("爬取失败,HTTP:" + str(r))
        else:
            title = getTitle(r.text)
            problemMD = getProblemMD(r.text)
            print("爬取成功！正在保存...",end="")
            saveData(problemMD,"P"+str(i)+"-"+str(title) +".md")
            print("题目保存成功!")
        r = getSolutionHTML(baseUrl +"/solution/P"+ str(i))
        if str(r).find("200") != 11:
            print("爬取失败,HTTP:" + str(r))
        else:
            #title = getTitle(r.text)
            problemMD = getSolutionMD(r.content)
            print("爬取成功！正在保存...",end="")
            saveData(problemMD,"P"+str(i)+"-"+str(title)+ "-题解" +".md")
            print("解答保存成功!")
    print("爬取完毕")
    return 0

def getProblemHTML(url):
    #伪装自己是浏览器
    headers = {
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36 Edg/116.0.1938.76"
    }
    r = requests.get(url = url,headers = headers)
    r.encoding = 'utf-8'
    if str(r).find("200") == 11:#正确返回的话状态码为200
        return r
    elif(str(r).find("302") == 11):
        return "302"
    elif(str(r).find("403") == 11):
        return "403"
    else:
        return "未知错误"
    

def getSolutionHTML(url):
    #不知道具体需要哪些头字段所以直接全写了
    headers = {
        "authority":"www.luogu.com.cn",
        "method":"GET",
        "path":"/problem/P8000",
        "scheme":"https",
        "Accept":"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "Accept-Encoding":"gzip, deflate, br",
        "Accept-Language":"zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6",
        "Cookie":"__client_id=a9a7f6dc9a1733e681d82f395f7d12d740413ff7; _uid=1088102; C3VK=195332",
        "Referer":"https://www.luogu.com.cn/",
        #"Sec-Ch-Ua":"Chromium";v="116", "Not)A;Brand";v="24", "Microsoft Edge";v="116"
        #Sec-Ch-Ua-Mobile:?0
        #Sec-Ch-Ua-Platform:"Windows"
        #Sec-Fetch-Dest:document
        #Sec-Fetch-Mode:navigate
        #Sec-Fetch-Site:same-origin
        #Sec-Fetch-User:?1
        "Upgrade-Insecure-Requests":"1",
        "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36 Edg/116.0.1938.76"
    }
    r = requests.get(url = url,headers = headers)
    r.encoding = 'ISO-8859-1'
    #print(str(r))
    if str(r).find("200") == 11:#正确返回的话状态码为200
        return r
    elif(str(r).find("302") == 11):
        return "302"
    elif(str(r).find("403") == 11):
        return "403"
    else:
        return "未知错误"
def getTitle(text):
    bs = bs4.BeautifulSoup(text,"html.parser")
    #通过<h1>标签确定标题位置
    try:
        text = bs.select("h1")[0]
    except:
        print(str(text))
        text = bs.select("h3")[0]
    print(str(text))
    #剪切字符串
    title = str(text).removeprefix("<h1>")
    title = title.removesuffix("</h1>")
    return title

def getProblemMD(text):
    bs = bs4.BeautifulSoup(text,"html.parser")
    core = bs.select("article")[0]
    md = str(core)
    md = re.sub("<h1>","# ",md)
    md = re.sub("<h2>","## ",md)
    md = re.sub("<h3>","#### ",md)
    md = re.sub("</?[a-zA-Z]+[^<>]*>","",md)
    return md

def getSolutionMD(text):
    #解码
    textsl = str(urllib.parse.unquote(text,encoding ='unicode_escape'))
    bs = bs4.BeautifulSoup(textsl,"html.parser")
    #core = bs.select("script")[0]
    core = bs.find("script")
    md = str(core)
    #print(md)
    md = re.sub("<h1>","# ",md)
    md = re.sub("<h2>","## ",md)
    md = re.sub("<h3>","#### ",md)
    md = re.sub("</?[a-zA-Z]+[^<>]*>","",md)
    return md

def saveData(data,filename):
    cfilename = savePath + filename
    file = open(cfilename,"w",encoding="utf-8")
    for d in data:
        file.writelines(d)
    file.close()

if __name__ == '__main__':
    main()



```



#### 预估哪几个子任务可以利用AIGC

了解了一下VScode下的Github Copilot,发现其功能虽然强大,但还是有一点使用门槛的

#### 实际中哪些部分利用了AIGC

#### AIGC技术的优缺点,适合用在哪些方面

缺点:贵

优点:

爬下来的题目用Markdown格式存储,命名为**题目编号-标题.md**,对应题解用**题目编号-标题-题解.md**一起放入**题目编号-标题**文件夹下

项目要传到Github仓库

``raise JSONDecodeError("Expecting value", s, err.value) from None``

load方法使用错误

``UnicodeDecodeError: 'gbk' codec can't decode byte 0xad in position 28: illegal multibyte sequence``

编码方法有问题

使用re包的时候需要添加re.S字段来将整个text视为同一字符串

出现python在open时不会自动创建文件(==其实是文件夹==)问题

权限不足Python无法直接创建文件夹,找了半天解决不了前端还没做先去做前端了

``json.decoder.JSONDecodeError: Invalid \escape:``

需要在json获取的时候就将转义符进行处理

Tkinter button comand绑定的函数在窗口形成之前就会自己跑,需要使用lamda表达式来进行绑定

``TypeError: __main__.main() argument after * must be an iterable, not int``

threading传值只能传入可迭代对象,通过将整数转化为元组解决

``Exception has occurred: OSError[Errno 22] Invalid argument:``

在使用多线程之后,时常会触发HTTP403拒绝访问的问题,
